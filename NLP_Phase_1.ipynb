{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "NLP PHASE 1"
      ],
      "metadata": {
        "id": "RSXuhLXXSGoS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WK8vLcTPSAJs"
      },
      "outputs": [],
      "source": [
        "#TEXT PROCESSING\n",
        "#text-> cleaning-> tokenization -> stopwords removal-> stemming"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"I Am Learning PYTHON programming!!!\"\n",
        "print(\"original text: \",text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnqjZLkhSaLR",
        "outputId": "41dae86f-e8e9-4a9b-952c-312d5adffcfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original text:  I Am Learning PYTHON programming!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#step 1-> CONVERT TO LOWERCASE\n",
        "text=text.lower()\n",
        "print(\"Lowercase text: \",text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCV-yN2MSmCy",
        "outputId": "9a90a520-0186-440d-ade7-ff8f92030bb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lowercase text:  i am learning python programming!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#STEP 2-> REMOVE PUNCTUATION\n",
        "#re - regex - regular expression\n",
        "import re\n",
        "text=re.sub(r'[^\\w\\s]','',text)\n",
        "print(\"Without punctuation: \",text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLP-zbMMSyRP",
        "outputId": "08aef5f2-7e88-4e93-8379-c6742e8efcb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without punctuation:  i am learning python programming\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenization\n",
        "tokens=text.split()\n",
        "print(\"Tokens :\",tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmwbxM_6UjxU",
        "outputId": "69106f80-c23c-4aa2-dfd8-77459cb467d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens : ['i', 'am', 'learning', 'python', 'programming']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#stopwords- is, am,are,the,a,an\n",
        "#removestopwords\n",
        "stopwords=['a','am','is','i','the','and']\n",
        "filtered_tokens=[]\n",
        "for word in tokens:\n",
        "    if word not in stopwords:\n",
        "        filtered_tokens.append(word)\n",
        "print(\"After removing stopwords:\",filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHmI1TPlXZb-",
        "outputId": "69c58c8f-321d-4546-fd4b-9e56c931a67a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After removing stopwords: ['learning', 'python', 'programming']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#STEMMING\n",
        "def stem(word):\n",
        "    if word.endswith(\"ing\"):\n",
        "        return word[:-3]\n",
        "    return word\n",
        "stemmed_words=[]\n",
        "for word in filtered_tokens:\n",
        "    stemmed_words.append(stem(word))\n",
        "print(\"After stemming:\",stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A73e30HhXtz6",
        "outputId": "44742411-faea-4007-b1e5-3d57a7be958c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After stemming: ['learn', 'python', 'programm']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLP PHASE 2: WORD TO VECTORS"
      ],
      "metadata": {
        "id": "Tv_3Ss6jYq9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#why vectors?\n",
        "#ML MODELS UNDERSTAND ONLY NUMBERS\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "5NFdNuaSYWHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences=[\n",
        "    \"i am learning python\",\n",
        "    \"we are happy today\",\n",
        "    \"i am sad today\"\n",
        "]"
      ],
      "metadata": {
        "id": "_OrKrWeTZM-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer=CountVectorizer()#create objects\n",
        "X=vectorizer.fit_transform(sentences)\n",
        "#learns vocabulary\n",
        "#convert sentences to numbers\n",
        "print(\"Vocabulary:\",vectorizer.get_feature_names_out())\n",
        "#1.all unique words\n",
        "#2.sorted in alphabetical order\n",
        "#3.remove single letters like i,a,"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3WNAgSQZnCJ",
        "outputId": "279cf8d4-fb61-4e6e-dcef-29a839440db9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['am' 'are' 'happy' 'learning' 'python' 'sad' 'today' 'we']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vectors:\")\n",
        "print(X.toarray())\n",
        "#each row is a sentence\n",
        "#each column is a word\n",
        "#eaxch value is a count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oovj6WI8aiL-",
        "outputId": "bcca5d9a-8ec6-4670-8758-76b4bae480dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectors:\n",
            "[[1 0 0 1 1 0 0 0]\n",
            " [0 1 1 0 0 0 1 1]\n",
            " [1 0 0 0 0 1 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF = TERM FREQUENCY"
      ],
      "metadata": {
        "id": "gQNnpO6Zc0n3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#WHY TDF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "mwTf_jQEbdfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences=[\n",
        "    \"i am learning python\",\n",
        "    \"we are happy today\",\n",
        "    \"i am sad today\"\n",
        "]"
      ],
      "metadata": {
        "id": "e0U55kXmdFEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer=TfidfVectorizer()\n",
        "X=vectorizer.fit_transform(sentences)\n",
        "#build vocab\n",
        "#calculate TF\n",
        "#calculate idf- impact of each word\n",
        "#multiples=tf x idf"
      ],
      "metadata": {
        "id": "0EDrcZIodJ1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"TF-IDF MATXIX: \")\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlmrFQ3JdlJ1",
        "outputId": "ea1ee1ea-b4aa-4f0f-9e12-70fb0463151b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF MATXIX: \n",
            "[[0.4736296  0.         0.         0.62276601 0.62276601 0.\n",
            "  0.         0.        ]\n",
            " [0.         0.52863461 0.52863461 0.         0.         0.\n",
            "  0.40204024 0.52863461]\n",
            " [0.51785612 0.         0.         0.         0.         0.68091856\n",
            "  0.51785612 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#count+ impact of each words\n",
        "#TF-IDF rewards rare important words and penalizes common words"
      ],
      "metadata": {
        "id": "rw0XW3VqelrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TF = count(words)/total words\n",
        "#IDF=log(total_docs)"
      ],
      "metadata": {
        "id": "yeI1dEXTfSr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NIhM6antB7Uj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk   #NATURAL LANGUAGE TOOLKIT\n",
        "#4 THINGS TO DO\n",
        "#1. TOKENIZATION\n",
        "#2. STOPWORD REMOVAL\n",
        "#3. STEMMING\n",
        "#4. LEMMATIZATION"
      ],
      "metadata": {
        "id": "V_ddxiAzB0bO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUz93AJxB2yd",
        "outputId": "375ec676-f60a-4c8f-aa95-4123f90bb37f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TOKENIZATION\n",
        "from nltk.tokenize import word_tokenize\n",
        "#STOPWORDS LIST\n",
        "from nltk.corpus import stopwords\n",
        "#stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "#lemmatization\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "FBQoWEZ4C0zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text=\"I am learning python programming. Python is a great language!!!!!....\"\n",
        "#Step 1 LOWERCASE\n",
        "raw_text=raw_text.lower()\n",
        "print(\"After Lowercase: \",raw_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aL2bL8WbD4N0",
        "outputId": "aaae6b0f-a5a5-41f4-e863-b65fc8298ad7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Lowercase:  i am learning python programming. python is a great language!!!!!....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TOKENIZE\n",
        "print(raw_text.split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQpHswPyDffy",
        "outputId": "7a567388-39bc-4dfe-fc5d-3beb8b7afc92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'am', 'learning', 'python', 'programming.', 'python', 'is', 'a', 'great', 'language!!!!!....']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token=word_tokenize(raw_text)\n",
        "print(\"Tokens: \",token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mm7I1DjQEcQh",
        "outputId": "89c0eb0d-f185-4617-86db-901e47fad58c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:  ['i', 'am', 'learning', 'python', 'programming', '.', 'python', 'is', 'a', 'great', 'language', '!', '!', '!', '!', '!', '....']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#STEP 3: REMOVE STOPWORDS\n",
        "stop_words=set(stopwords.words('english'))\n",
        "filtered_tokens=[]\n",
        "for word in token:\n",
        "    if word not in stop_words:\n",
        "        filtered_tokens.append(word)\n",
        "print(\"Filtered token: \",filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rT-4aO9tE5qY",
        "outputId": "3e6d6b82-566f-48eb-a0eb-2ab3b36d51ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered token:  ['learning', 'python', 'programming', '.', 'python', 'great', 'language', '!', '!', '!', '!', '!', '....']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#STEP 4: REMOVE PUNCTUATIONS\n",
        "import string\n",
        "punctuations=string.punctuation\n",
        "print(\"Punctuation available: \",punctuations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "du_bJU5pF3qY",
        "outputId": "199d09f4-9516-4661-f700-5bea9b6bf52a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Punctuation available:  !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_tokens=[word for word in filtered_tokens if word not in punctuations]\n",
        "print(\"after removing punctuations: \",clean_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oAz7KrkHOQ6",
        "outputId": "5e9cd5dc-9c4c-4a60-802e-49b48df4039a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "after removing punctuations:  ['learning', 'python', 'programming', 'python', 'great', 'language', '....']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stem=PorterStemmer()\n",
        "stem.stem('learning')\n",
        "stem.stem('flying')\n",
        "stem.stem('bought')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0c3AS4b8HYVv",
        "outputId": "c9d045ab-1810-4d77-b828-9d3b3f1437d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'bought'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wnet=WordNetLemmatizer()\n",
        "#wnet.lemmatize('learning','v')\n",
        "#wnet.lemmatize('flying','v')\n",
        "#wnet.lemmatize('bought','v')\n",
        "wnet.lemmatize('went','v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wPfB0D8XIBZg",
        "outputId": "fba888fa-ca1f-4f67-d644-d9678b4838a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'go'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_words=[]\n",
        "for word in clean_tokens:\n",
        "    lemmatized_words.append(wnet.lemmatize(word,'v'))\n",
        "print(\"Lemmatized words: \",lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbkMA0bSIRMJ",
        "outputId": "01c07ce4-ef3b-4d64-b8f3-cefe591f2cb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized words:  ['learn', 'python', 'program', 'python', 'great', 'language', '....']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_tokens=[]\n",
        "for word in lemmatized_words:\n",
        "  if word.isalpha():\n",
        "    final_tokens.append(word)\n",
        "print(\"Final tokens: \",final_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2xmD2FhJYcB",
        "outputId": "50c6c08b-019e-4ef8-d789-9457f7eb6412"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final tokens:  ['learn', 'python', 'program', 'python', 'great', 'language']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_text=\" \".join(final_tokens)\n",
        "print(\"Clean text: \",clean_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FydWhDzAJyPM",
        "outputId": "092da2fa-7cae-4f9b-8fbb-d36f8c730942"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean text:  learn python program python great language\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences=[]\n"
      ],
      "metadata": {
        "id": "S8IuzxMYJ-Ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e8fzCvdVVWAL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}